{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9Qbsl/J3j4n+efby+Vksj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JithinBinoy-sudo/Improve-Downward-Monotonicity/blob/main/Downward_Monotonicity_Improvement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Tx96vCFtDgMz",
        "outputId": "84c6c0fe-6a51-40e3-a1cf-b6ea9e43dada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.2\n",
            "    Uninstalling transformers-4.55.2:\n",
            "      Successfully uninstalled transformers-4.55.2\n",
            "Successfully installed transformers-4.55.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "5e6c302de6e346729db4eaeae24e8888"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# ----------------------------\n",
        "# 1. DISABLE W&B LOGGING\n",
        "# ----------------------------\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ----------------------------\n",
        "# 2. DOWNLOAD HELP DATASET\n",
        "# ----------------------------\n",
        "if not os.path.exists(\"help_dataset.tsv\"):\n",
        "    os.system(\"wget https://github.com/verypluming/HELP/raw/master/output_en/pmb_train_v1.0.tsv -O help_dataset.tsv\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3. LOAD HELP DATASET\n",
        "# ----------------------------\n",
        "df_help = pd.read_csv(\"help_dataset.tsv\", sep=\"\\t\")\n",
        "df_help = df_help[['ori_sentence', 'new_sentence', 'gold_label']]\n",
        "df_help.columns = ['premise', 'hypothesis', 'label']\n",
        "\n",
        "label_mapping = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
        "df_help['label'] = df_help['label'].map(label_mapping)\n",
        "\n",
        "help_data = list(zip(df_help['premise'], df_help['hypothesis'], df_help['label']))\n",
        "print(f\"Loaded HELP dataset with {len(help_data)} examples.\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. POLARITY TAGGING\n",
        "# ----------------------------\n",
        "def add_polarity_tags(text):\n",
        "    text_lower = text.lower()\n",
        "    if any(word in text_lower for word in [\"no\",\"none\",\"never\",\"few\",\"at most\",\"not all\",\"without\",\"exactly\"]):\n",
        "        return \"[DOWN] \" + text\n",
        "    else:\n",
        "        return \"[UP] \" + text\n",
        "\n",
        "# ----------------------------\n",
        "# 5. CUSTOM LOGICAL DATASET (128 EXAMPLES)\n",
        "# ----------------------------\n",
        "sample_data = [\n",
        "    # --- ENTAILMENT (43) ---\n",
        "    (\"All cats are mammals\", \"Some cats are mammals\", 0),\n",
        "    (\"No dogs can fly\", \"Some dogs cannot fly\", 0),\n",
        "    (\"Every student passed the exam\", \"Some students passed\", 0),\n",
        "    (\"At least three birds are singing\", \"Some birds are singing\", 0),\n",
        "    (\"Each car has wheels\", \"All cars have wheels\", 0),\n",
        "    (\"All apples are red\", \"Some apples are red\", 0),\n",
        "    (\"Every teacher attended the meeting\", \"Some teachers attended\", 0),\n",
        "    (\"Not all birds can fly\", \"Some birds cannot fly\", 0),\n",
        "    (\"Exactly two players scored\", \"Two players scored\", 0),\n",
        "    (\"Every child got a gift\", \"Each child received a present\", 0),\n",
        "    (\"Some men are teachers\", \"Some people are teachers\", 0),\n",
        "    (\"All cars have wheels\", \"Some cars have wheels\", 0),\n",
        "    (\"Every dog barked loudly\", \"Some dogs barked\", 0),\n",
        "    (\"At least one window is open\", \"Some windows are open\", 0),\n",
        "    (\"Not all lights are on\", \"Some lights are off\", 0),\n",
        "    (\"Exactly three students passed\", \"Three students passed\", 0),\n",
        "    (\"Each room has a window\", \"Every room has a window\", 0),\n",
        "    (\"All birds can fly\", \"Some birds can fly\", 0),\n",
        "    (\"Some kids are laughing\", \"Some kids are happy\", 0),\n",
        "    (\"Every student joined\", \"All students joined\", 0),\n",
        "    (\"At least two chairs are broken\", \"Some chairs are broken\", 0),\n",
        "    (\"Some phones are charging\", \"Some devices are charging\", 0),\n",
        "    (\"No cats are swimming\", \"No felines are swimming\", 0),\n",
        "    (\"Each house has a roof\", \"Every house has a roof\", 0),\n",
        "    (\"Some dogs are barking\", \"Some animals are making noise\", 0),\n",
        "    (\"All students received books\", \"Some students received books\", 0),\n",
        "    (\"Every flower is blooming\", \"Some flowers are blooming\", 0),\n",
        "    (\"At most five chairs are broken\", \"At most six chairs are broken\", 0),\n",
        "    (\"Exactly one window is open\", \"One window is open\", 0),\n",
        "    (\"All cars have brakes\", \"Some cars have brakes\", 0),\n",
        "    (\"Every child is happy\", \"Some children are happy\", 0),\n",
        "    (\"Some trees are tall\", \"Some plants are tall\", 0),\n",
        "    (\"Not all birds sing\", \"Some birds are silent\", 0),\n",
        "    (\"All laptops are charged\", \"Some laptops are charged\", 0),\n",
        "    (\"Every student answered correctly\", \"Some students answered correctly\", 0),\n",
        "    (\"Some cats are playful\", \"Some animals are playful\", 0),\n",
        "    (\"All houses have doors\", \"Some houses have doors\", 0),\n",
        "    (\"Each person attended the meeting\", \"Every person attended\", 0),\n",
        "    (\"At most three students failed\", \"At most four students failed\", 0),\n",
        "    (\"Every bird can fly\", \"Some birds can fly\", 0),\n",
        "    (\"Some flowers are red\", \"Some plants are red\", 0),\n",
        "    (\"All cars are clean\", \"Some cars are clean\", 0),\n",
        "    (\"Every dog is barking\", \"Some dogs are barking\", 0),\n",
        "\n",
        "    # --- NEUTRAL (42) ---\n",
        "    (\"Few dogs barked\", \"Some dogs barked\", 1),\n",
        "    (\"Many cats slept\", \"All cats slept\", 1),\n",
        "    (\"Most students passed\", \"Every student passed\", 1),\n",
        "    (\"Some children are playing\", \"Some children are studying\", 1),\n",
        "    (\"Every dog barked\", \"Some dogs barked\", 1),\n",
        "    (\"At least 5 students joined\", \"At least 10 students joined\", 1),\n",
        "    (\"John visited Paris\", \"Mary visited London\", 1),\n",
        "    (\"Some books are new\", \"Some chairs are new\", 1),\n",
        "    (\"Many birds are singing\", \"All birds are flying\", 1),\n",
        "    (\"At least one car stopped\", \"At least three cars stopped\", 1),\n",
        "    (\"Some people are dancing\", \"Some people are talking\", 1),\n",
        "    (\"Every child laughed\", \"Some children cried\", 1),\n",
        "    (\"A few dogs barked\", \"A few cats meowed\", 1),\n",
        "    (\"Some laptops are on\", \"Some tablets are on\", 1),\n",
        "    (\"All chairs are brown\", \"All tables are brown\", 1),\n",
        "    (\"Most kids enjoyed the show\", \"Most kids disliked the show\", 1),\n",
        "    (\"The store opens at 8 AM\", \"The store closes at 8 PM\", 1),\n",
        "    (\"Some players scored\", \"Some coaches cheered\", 1),\n",
        "    (\"He went to school\", \"She stayed home\", 1),\n",
        "    (\"Some streets are closed\", \"Some houses are closed\", 1),\n",
        "    (\"Every tree is tall\", \"Some trees are tall\", 1),\n",
        "    (\"A few birds are flying\", \"Some animals are flying\", 1),\n",
        "    (\"Most chairs are occupied\", \"Some chairs are empty\", 1),\n",
        "    (\"At least three doors are open\", \"Some doors are open\", 1),\n",
        "    (\"Some people are walking\", \"Some people are running\", 1),\n",
        "    (\"Every child is smiling\", \"Some children are frowning\", 1),\n",
        "    (\"Some phones are off\", \"Some devices are on\", 1),\n",
        "    (\"Most students studied\", \"Some students did not study\", 1),\n",
        "    (\"A few cats are sleeping\", \"Some animals are sleeping\", 1),\n",
        "    (\"Some windows are closed\", \"Some doors are closed\", 1),\n",
        "    (\"Every dog is awake\", \"Some dogs are asleep\", 1),\n",
        "    (\"Some tables are round\", \"Some chairs are round\", 1),\n",
        "    (\"Most flowers are blooming\", \"Some flowers are not blooming\", 1),\n",
        "    (\"At least two cars stopped\", \"Some cars stopped\", 1),\n",
        "    (\"Some people are talking\", \"Some people are listening\", 1),\n",
        "    (\"Every student is present\", \"Some students are absent\", 1),\n",
        "    (\"Some birds are chirping\", \"Some animals are making noise\", 1),\n",
        "    (\"Most laptops are working\", \"Some laptops are broken\", 1),\n",
        "    (\"A few windows are open\", \"Some doors are open\", 1),\n",
        "    (\"Some children are running\", \"Some children are walking\", 1),\n",
        "    (\"Most lights are on\", \"Some lights are off\", 1),\n",
        "    (\"Some chairs are broken\", \"Some tables are broken\", 1),\n",
        "    (\"Every phone is charged\", \"Some phones are not charged\", 1),\n",
        "\n",
        "    # --- CONTRADICTION (43) ---\n",
        "    (\"No cars are electric\", \"Some cars are electric\", 2),\n",
        "    (\"Every student passed\", \"Some students did not pass\", 2),\n",
        "    (\"Some children are playing\", \"No children are playing\", 2),\n",
        "    (\"Without any help, she succeeded\", \"She did not succeed\", 2),\n",
        "    (\"All cats are black\", \"No cats are black\", 2),\n",
        "    (\"Most birds can fly\", \"No birds can fly\", 2),\n",
        "    (\"Exactly one student attended\", \"No student attended\", 2),\n",
        "    (\"She is alive\", \"She is dead\", 2),\n",
        "    (\"No one entered the room\", \"Someone entered the room\", 2),\n",
        "    (\"Every light is off\", \"Some lights are on\", 2),\n",
        "    (\"No apples are red\", \"Some apples are red\", 2),\n",
        "    (\"All students are happy\", \"No students are happy\", 2),\n",
        "    (\"Most chairs are broken\", \"No chairs are broken\", 2),\n",
        "    (\"Every dog is barking\", \"No dog is barking\", 2),\n",
        "    (\"Some windows are open\", \"No windows are open\", 2),\n",
        "    (\"All birds are singing\", \"No birds are singing\", 2),\n",
        "    (\"The box is empty\", \"The box is full\", 2),\n",
        "    (\"She passed the test\", \"She failed the test\", 2),\n",
        "    (\"No phones are charging\", \"Some phones are charging\", 2),\n",
        "    (\"He is present\", \"He is absent\", 2),\n",
        "    (\"All tables are round\", \"No tables are round\", 2),\n",
        "    (\"Some people are outside\", \"Nobody is outside\", 2),\n",
        "    (\"No dogs are barking\", \"Some dogs are barking\", 2),\n",
        "    (\"Every cat is sleeping\", \"Some cats are awake\", 2),\n",
        "    (\"No students attended\", \"Some students attended\", 2),\n",
        "    (\"All birds are flying\", \"No birds are flying\", 2),\n",
        "    (\"Every light is on\", \"Some lights are off\", 2),\n",
        "    (\"No chairs are broken\", \"Some chairs are broken\", 2),\n",
        "    (\"All phones are off\", \"Some phones are on\", 2),\n",
        "    (\"Every tree is tall\", \"Some trees are short\", 2),\n",
        "    (\"No windows are open\", \"Some windows are open\", 2),\n",
        "    (\"All doors are closed\", \"Some doors are open\", 2),\n",
        "    (\"Every student is late\", \"Some students are on time\", 2),\n",
        "    (\"No birds are singing\", \"Some birds are singing\", 2),\n",
        "    (\"All laptops are off\", \"Some laptops are on\", 2),\n",
        "    (\"Every child is crying\", \"Some children are laughing\", 2),\n",
        "    (\"No cats are playing\", \"Some cats are playing\", 2),\n",
        "    (\"All flowers are dead\", \"Some flowers are alive\", 2),\n",
        "    (\"Every dog is asleep\", \"Some dogs are awake\", 2),\n",
        "    (\"No students passed\", \"Some students passed\", 2),\n",
        "    (\"All tables are broken\", \"Some tables are fine\", 2),\n",
        "    (\"Every phone is broken\", \"Some phones are working\", 2),\n",
        "    (\"No lights are on\", \"Some lights are on\", 2),\n",
        "    (\"All birds are dead\", \"Some birds are alive\", 2),\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6. BALANCE HELP DATASET\n",
        "# ----------------------------\n",
        "counts = Counter([label for _, _, label in sample_data])\n",
        "min_count = min(counts.values())\n",
        "\n",
        "help_by_label = {0: [], 1: [], 2: []}\n",
        "for p, h, l in help_data:\n",
        "    help_by_label[l].append((p, h, l))\n",
        "\n",
        "help_balanced = []\n",
        "for l in [0,1,2]:\n",
        "    help_balanced.extend(random.sample(help_by_label[l], min(min_count, len(help_by_label[l]))))\n",
        "\n",
        "print(f\"Balanced HELP dataset size: {len(help_balanced)}\")\n",
        "counts_help = Counter([label for _, _, label in help_balanced])\n",
        "print(f\"Class distribution in balanced HELP dataset: {counts_help}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7. MERGE CUSTOM + HELP\n",
        "# ----------------------------\n",
        "all_data = sample_data + help_balanced\n",
        "all_data = [(add_polarity_tags(p), add_polarity_tags(h), l) for p,h,l in all_data]\n",
        "\n",
        "# ----------------------------\n",
        "# 8. TRAIN/TEST SPLIT\n",
        "# ----------------------------\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    [(p,h) for p,h,l in all_data],\n",
        "    [l for _,_,l in all_data],\n",
        "    test_size=0.1,\n",
        "    stratify=[l for _,_,l in all_data],\n",
        "    random_state=42\n",
        ")\n",
        "train_data = [(p,h,l) for (p,h),l in zip(train_texts, train_labels)]\n",
        "test_data  = [(p,h,l) for (p,h),l in zip(test_texts, test_labels)]\n",
        "\n",
        "# ----------------------------\n",
        "# 9. TOKENIZER\n",
        "# ----------------------------\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[UP]\", \"[DOWN]\"]})\n",
        "\n",
        "# ----------------------------\n",
        "# 10. DATASET CLASS\n",
        "# ----------------------------\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MonotonicityDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, h, label = self.data[idx]\n",
        "        enc = self.tokenizer(\n",
        "            p, h,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_ds = MonotonicityDataset(train_data, tokenizer)\n",
        "test_ds  = MonotonicityDataset(test_data, tokenizer)\n",
        "\n",
        "# ----------------------------\n",
        "# 11. MODEL\n",
        "# ----------------------------\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# ----------------------------\n",
        "# 12. METRICS\n",
        "# ----------------------------\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# 13. TRAINING\n",
        "# ----------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Training started...\")\n",
        "trainer.train()\n",
        "\n",
        "# ----------------------------\n",
        "# 14. EVALUATION\n",
        "# ----------------------------\n",
        "results = trainer.evaluate(eval_dataset=test_ds)\n",
        "print(\"Overall Evaluation:\", results)\n",
        "\n",
        "# ----------------------------\n",
        "# 15. UP/DOWN ACCURACY REPORTING\n",
        "# ----------------------------\n",
        "def polarity_metrics(trainer, dataset):\n",
        "    up_preds, up_labels = [], []\n",
        "    down_preds, down_labels = [], []\n",
        "\n",
        "    for item in dataset:\n",
        "        input_ids = item['input_ids'].unsqueeze(0)\n",
        "        attention_mask = item['attention_mask'].unsqueeze(0)\n",
        "        label = item['labels'].item()\n",
        "        logits = trainer.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        pred = logits.argmax(dim=-1).item()\n",
        "\n",
        "        first_token_id = input_ids[0,1].item()  # index 1 is [UP]/[DOWN]\n",
        "        if first_token_id == tokenizer.convert_tokens_to_ids(\"[UP]\"):\n",
        "            up_preds.append(pred)\n",
        "            up_labels.append(label)\n",
        "        else:\n",
        "            down_preds.append(pred)\n",
        "            down_labels.append(label)\n",
        "\n",
        "    print(f\"UP Accuracy: {accuracy_score(up_labels, up_preds):.2f}\")\n",
        "    print(f\"DOWN Accuracy: {accuracy_score(down_labels, down_preds):.2f}\")\n",
        "\n",
        "polarity_metrics(trainer, test_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EcXsQUlN8FXE",
        "outputId": "6a034243-f5f7-4d62-ec0d-66f3832143b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded HELP dataset with 35891 examples.\n",
            "Balanced HELP dataset size: 86\n",
            "Class distribution in balanced HELP dataset: Counter({0: 43, 1: 43})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-2229632194.py:293: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [490/490 15:00, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.019700</td>\n",
              "      <td>0.961019</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.193548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.993100</td>\n",
              "      <td>0.884485</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.786500</td>\n",
              "      <td>0.802888</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.599567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.659200</td>\n",
              "      <td>0.769332</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.627350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.501500</td>\n",
              "      <td>0.789682</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.679739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.247800</td>\n",
              "      <td>0.967043</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.574675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.184500</td>\n",
              "      <td>0.888950</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.753813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.279300</td>\n",
              "      <td>0.910888</td>\n",
              "      <td>0.772727</td>\n",
              "      <td>0.790850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.969113</td>\n",
              "      <td>0.772727</td>\n",
              "      <td>0.790850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.043100</td>\n",
              "      <td>1.026856</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.730908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Evaluation: {'eval_loss': 1.0268563032150269, 'eval_accuracy': 0.7272727272727273, 'eval_f1_macro': 0.730908152734778, 'eval_runtime': 2.3694, 'eval_samples_per_second': 9.285, 'eval_steps_per_second': 1.266, 'epoch': 10.0}\n",
            "UP Accuracy: 0.67\n",
            "DOWN Accuracy: 0.86\n"
          ]
        }
      ]
    }
  ]
}